{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cloud.google.com/ai-platform/training/docs/getting-started-scikit-xgboost\n",
    "\n",
    "https://cloud.google.com/ai-platform/prediction/docs/using-pipelines-for-preprocessing#gcloud_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# modulePath = '/Users/kristine/WRI/MachineLearning/gfw-forestlearn'\n",
    "# if modulePath not in sys.path:\n",
    "#     sys.path.append(modulePath)\n",
    "# from gfw_forestlearn import fl_binary_classification\n",
    "# #from gfw_forestlearn import fl_regression\n",
    "# #import pandas as pd\n",
    "# import numpy as np\n",
    "# from datetime import datetime\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import subprocess\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = '/Users/kristine/Downloads/CongoGEE'\n",
    "os.chdir(dataDir)\n",
    "trainFile = 'train.csv'\n",
    "testFile = 'test.csv'\n",
    "validationFile = 'validation.csv'\n",
    "\n",
    "train = pd.read_csv(trainFile)\n",
    "validation = pd.read_csv(validationFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'lossyear'\n",
    "predictors = ['concessionsImage','conflictsDistance','earlyLossDistance','elevation', 'huntingAreasImage',\n",
    "              'localitiesDistance', 'loggingDistance', 'lossyear', 'miningImage', 'navigableRiversDistance',\n",
    "              'protectedAreasImage', 'roadsDistance', 'ruralComplexDistance', 'slope']\n",
    "\n",
    "cat_feats = ['concessionsImage','miningImage','huntingAreasImage','protectedAreasImage']\n",
    "\n",
    "all_cols = [label]+predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system:index', 'SplitValue', 'concessionsImage', 'conflictsDistance', 'earlyLossDistance', 'elevation', 'huntingAreasImage', 'localitiesDistance', 'loggingDistance', 'lossyear', 'miningImage', 'navigableRiversDistance', 'protectedAreasImage', 'random', 'roadsDistance', 'ruralComplexDistance', 'slope', '.geo']\n",
      "['concessionsImage', 'conflictsDistance', 'earlyLossDistance', 'elevation', 'huntingAreasImage', 'localitiesDistance', 'loggingDistance', 'lossyear', 'miningImage', 'navigableRiversDistance', 'protectedAreasImage', 'roadsDistance', 'ruralComplexDistance', 'slope']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tensorflow_decision_forests/keras/core.py:2036: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  features_dataframe = dataframe.drop(label, 1)\n",
      "2022-02-02 15:39:27.925062: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Name of the label column.\n",
    "\n",
    "\n",
    "print(list(train))\n",
    "#print(train[label].unique())\n",
    "trainFilt = train.drop(columns=[x for x in list(train) if x not in all_cols])\n",
    "validationFilt = train.drop(columns=[x for x in list(validation) if x not in all_cols])\n",
    "\n",
    "print(list(trainFilt))\n",
    "#print(train[label].unique())\n",
    "\n",
    "train_tf = tfdf.keras.pd_dataframe_to_tf_dataset(trainFilt, label=label)\n",
    "validation_tf = tfdf.keras.pd_dataframe_to_tf_dataset(validationFilt, label=label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use /var/folders/ls/m3fnvjqj1qs_hsp9k3s_q1h00000gp/T/tmp6c_r4hs5 as temporary training directory\n",
      "Starting reading the dataset\n",
      "1/5 [=====>........................] - ETA: 14s\n",
      "Dataset read in 0:00:03.759500\n",
      "Training model\n",
      "Model trained in 0:00:00.483239\n",
      "Compiling model\n",
      "5/5 [==============================] - 4s 169ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO kernel.cc:1153] Loading model from path\n",
      "[INFO decision_forest.cc:617] Model loaded with 300 root(s), 59776 node(s), and 13 input feature(s).\n",
      "[INFO abstract_model.cc:1063] Engine \"RandomForestOptPred\" built\n",
      "[INFO kernel.cc:1001] Use fast generic engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x153d9a670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x153d9a670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x153d9a670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.9808\n",
      "loss: 0.0000\n",
      "accuracy: 0.9808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-02 15:39:35.050432: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) concessionsImage, conflictsDistance, earlyLossDistance, huntingAreasImage, localitiesDistance, loggingDistance, miningImage, navigableRiversDistance, protectedAreasImage, roadsDistance, ruralComplexDistance with unsupported characters which will be renamed to concessionsimage, conflictsdistance, earlylossdistance, huntingareasimage, localitiesdistance, loggingdistance, miningimage, navigableriversdistance, protectedareasimage, roadsdistance, ruralcomplexdistance in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as call_get_leaves while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify the model.\n",
    "model_1 = tfdf.keras.RandomForestModel()#features=predictors,exclude_non_specified_features=True)\n",
    "# Optionally, add evaluation metrics.\n",
    "model_1.compile(metrics=[\"accuracy\"]) #\n",
    "\n",
    "# Train the model.\n",
    "# \"sys_pipes\" is optional. It enables the display of the training logs.\n",
    "model_1.fit(x=train_tf)\n",
    "\n",
    "evaluation = model_1.evaluate(validation_tf, return_dict=True)\n",
    "\n",
    "for name, value in evaluation.items():\n",
    "    print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "model_1.save(\"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.tools import saved_model_utils\n",
    "\n",
    "# meta_graph_def = saved_model_utils.get_meta_graph_def('model', 'serve')\n",
    "# inputs = meta_graph_def.signature_def['serving_default'].inputs\n",
    "# outputs = meta_graph_def.signature_def['serving_default'].outputs\n",
    "\n",
    "# # Just get the first thing(s) from the serving signature def.  i.e. this\n",
    "# # model only has a single input and a single output.\n",
    "# input_name = None\n",
    "# for k,v in inputs.items():\n",
    "#     input_name = v.name\n",
    "#     #break\n",
    "\n",
    "# output_name = None\n",
    "# for k,v in outputs.items():\n",
    "#     output_name = v.name\n",
    "#     #break\n",
    "\n",
    "# # Make a dictionary that maps Earth Engine outputs and inputs to\n",
    "# # AI Platform inputs and outputs, respectively.\n",
    "# import json\n",
    "# input_dict = \"'\" + json.dumps({input_name: \"array\"}) + \"'\"\n",
    "# output_dict = \"'\" + json.dumps({output_name: \"output\"}) + \"'\"\n",
    "# print(input_dict)\n",
    "# print(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfdf.model_plotter.plot_model_in_colab(model_1)#, tree_idx=0, max_depth=3)\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItemsView({'elevation': name: \"serving_default_elevation:0\"\n",
      "dtype: DT_INT64\n",
      "tensor_shape {\n",
      "  dim {\n",
      "    size: -1\n",
      "  }\n",
      "}\n",
      ", 'ruralComplexDistance': name: \"serving_default_ruralComplexDistance:0\"\n",
      "dtype: DT_FLOAT\n",
      "tensor_shape {\n",
      "  dim {\n",
      "    size: -1\n",
      "  }\n",
      "}\n",
      ", 'miningImage': name: \"serving_default_miningImage:0\"\n",
      "dtype: DT_INT64\n",
      "tensor_shape {\n",
      "  dim {\n",
      "    size: -1\n",
      "  }\n",
      "}\n",
      ", 'navigableRiversDistance': name: \"serving_default_navigableRiversDistance:0\"\n",
      "dtype: DT_FLOAT\n",
      "tensor_shape {\n",
      "  dim {\n",
      "    size: -1\n",
      "  }\n",
      "}\n",
      ", 'localitiesDistance': name: \"serving_default_localitiesDistance:0\"\n",
      "dtype: DT_FLOAT\n",
      "tensor_shape {\n",
      "  dim {\n",
      "    size: -1\n",
      "  }\n",
      "}\n",
      ", 'protectedAreasImage': name: \"serving_default_protectedAreasImage:0\"\n",
      "dtype: DT_INT64\n",
      "tensor_shape {\n",
      "  dim {\n",
      "    size: -1\n",
      "  }\n",
      "}\n",
      ", 'slope': name: \"serving_default_slope:0\"\n",
      "dtype: DT_FLOAT\n",
      "tensor_shape {\n",
      "  dim {\n",
      "    size: -1\n",
      "  }\n",
      "}\n",
      ", 'roadsDistance': name: \"serving_default_roadsDistance:0\"\n",
      "dtype: DT_FLOAT\n",
      "tensor_shape {\n",
      "  dim {\n",
      "    size: -1\n",
      "  }\n",
      "}\n",
      ", 'concessionsImage': name: \"serving_default_concessionsImage:0\"\n",
      "dtype: DT_INT64\n",
      "tensor_shape {\n",
      "  dim {\n",
      "    size: -1\n",
      "  }\n",
      "}\n",
      ", 'earlyLossDistance': name: \"serving_default_earlyLossDistance:0\"\n",
      "dtype: DT_FLOAT\n",
      "tensor_shape {\n",
      "  dim {\n",
      "    size: -1\n",
      "  }\n",
      "}\n",
      ", 'huntingAreasImage': name: \"serving_default_huntingAreasImage:0\"\n",
      "dtype: DT_INT64\n",
      "tensor_shape {\n",
      "  dim {\n",
      "    size: -1\n",
      "  }\n",
      "}\n",
      ", 'conflictsDistance': name: \"serving_default_conflictsDistance:0\"\n",
      "dtype: DT_FLOAT\n",
      "tensor_shape {\n",
      "  dim {\n",
      "    size: -1\n",
      "  }\n",
      "}\n",
      ", 'loggingDistance': name: \"serving_default_loggingDistance:0\"\n",
      "dtype: DT_FLOAT\n",
      "tensor_shape {\n",
      "  dim {\n",
      "    size: -1\n",
      "  }\n",
      "}\n",
      "})\n",
      "'{\"serving_default_elevation:0\": \"array\"}'\n",
      "'{\"StatefulPartitionedCall_1:0\": \"output\"}'\n",
      "WARNING:root:TF Parameter Server distributed training not available (this is expected for the pre-build release).\n",
      "Successfully saved project id\n",
      "WARNING:root:TF Parameter Server distributed training not available (this is expected for the pre-build release).\n",
      "Warning: TensorFlow Addons not found. Models that use non-standard ops may not work.\n",
      "[INFO kernel.cc:1153] Loading model from path\n",
      "[INFO decision_forest.cc:617] Model loaded with 300 root(s), 59776 node(s), and 13 input feature(s).\n",
      "[INFO abstract_model.cc:1063] Engine \"RandomForestOptPred\" built\n",
      "[INFO kernel.cc:1001] Use fast generic engine\n",
      "Success: model at 'gs://congo_deforestation/ee_model' is ready to be hosted in AI Platform.\n"
     ]
    }
   ],
   "source": [
    "# Put the EEified model next to the trained model directory.\n",
    "MODEL_DIR = 'gs://congo_deforestation/model'\n",
    "\n",
    "from tensorflow.python.tools import saved_model_utils\n",
    "\n",
    "meta_graph_def = saved_model_utils.get_meta_graph_def(MODEL_DIR, 'serve')\n",
    "inputs = meta_graph_def.signature_def['serving_default'].inputs\n",
    "outputs = meta_graph_def.signature_def['serving_default'].outputs\n",
    "\n",
    "# Just get the first thing(s) from the serving signature def.  i.e. this\n",
    "# model only has a single input and a single output.\n",
    "input_name = None\n",
    "print(inputs.items())\n",
    "for k,v in inputs.items():\n",
    "    input_name = v.name\n",
    "    break\n",
    "\n",
    "output_name = None\n",
    "for k,v in outputs.items():\n",
    "    output_name = v.name\n",
    "    break\n",
    "\n",
    "# Make a dictionary that maps Earth Engine outputs and inputs to \n",
    "# AI Platform inputs and outputs, respectively.\n",
    "import json\n",
    "input_dict = \"'\" + json.dumps({input_name: \"array\"}) + \"'\"\n",
    "output_dict = \"'\" + json.dumps({output_name: \"output\"}) + \"'\"\n",
    "print(input_dict)\n",
    "print(output_dict)\n",
    "\n",
    "EEIFIED_DIR = 'gs://congo_deforestation/ee_model'\n",
    "PROJECT = 'drivers-deforestation2'\n",
    "# You need to set the project before using the model prepare command.\n",
    "!earthengine set_project {PROJECT}\n",
    "!earthengine model prepare --source_dir {MODEL_DIR} --dest_dir {EEIFIED_DIR} --input {input_dict} --output {output_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.models.create) Resource in projects [drivers-deforestation2] is the subject of a conflict: Field: model.name Error: A model with the same name already exists.\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: A model with the same name already exists.\n",
      "    field: model.name\n",
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.versions.create) ALREADY_EXISTS: Field: version.name Error: A version with the same name already exists.\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: A version with the same name already exists.\n",
      "    field: version.name\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'congo_model2'\n",
    "VERSION_NAME = 'v0'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "\n",
    "# !gcloud ai-platform models create {MODEL_NAME} \\\n",
    "#   --project {PROJECT} \\\n",
    "#   --region {REGION} \\\n",
    "#   --enable-logging\n",
    "\n",
    "!gcloud ai-platform versions create {VERSION_NAME} \\\n",
    "  --project {PROJECT} \\\n",
    "  --region {REGION} \\\n",
    "  --model {MODEL_NAME} \\\n",
    "  --origin {EEIFIED_DIR} \\\n",
    "  --framework \"TENSORFLOW\" \\\n",
    "  --runtime-version=2.7 \\\n",
    "  --python-version=3.7 \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [END upload-model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
