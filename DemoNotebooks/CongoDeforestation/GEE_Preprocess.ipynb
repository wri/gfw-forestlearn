{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import ee\n",
    "import geemap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "from ipyleaflet import Map, basemaps\n",
    "import random\n",
    "import time\n",
    "modulePath = '/Users/kristine/WRI/MachineLearning/gfw-forestlearn'\n",
    "if modulePath not in sys.path:\n",
    "    sys.path.append(modulePath)\n",
    "from gfw_forestlearn import ee_functions as eef\n",
    "\n",
    "#continuous historical prediction\n",
    "#projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/Results/DRC_Deforestation_Historical_Risk_Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Initialize earth engine\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception as e:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Image', 'bands': [{'id': 'lossyear', 'data_type': {'type': 'PixelType', 'precision': 'int', 'min': 0, 'max': 1}, 'crs': 'EPSG:4326', 'crs_transform': [0.00025, 0, -180, 0, -0.00025, 80]}]}\n",
      "['treecover2000']\n",
      "CRS and Transform:  EPSG:4326 [0.00025, 0, -180, 0, -0.00025, 80]\n"
     ]
    }
   ],
   "source": [
    "targetLoss = eef.getTargetLoss()\n",
    "\n",
    "# load Hansen forest change data\n",
    "forestChange = ee.Image(\"UMD/hansen/global_forest_change_2019_v1_7\")\n",
    "\n",
    "# save projection information\n",
    "projection_ee = forestChange.projection()\n",
    "projection = projection_ee.getInfo()\n",
    "crs = projection.get('crs')\n",
    "crsTransform = projection.get('transform')\n",
    "scale = projection_ee.nominalScale().getInfo()\n",
    "print('CRS and Transform: ',crs, crsTransform)\n",
    "\n",
    "# Load DRC feature\n",
    "countryBoundaries = ee.FeatureCollection('projects/resource-watch-gee/gadm36_0')\n",
    "DRCBoundary = ee.FeatureCollection('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/Boundaries/adm1_tshuapa')#countryBoundaries.filterMetadata('GID_0','equals','COD').first().geometry()\n",
    "\n",
    "\n",
    "seed=42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3931ea4d589e41ab928ae2bf7b225ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[-1.776125, 23.710125], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBoxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load primary forest data\n",
    "primaryForest = ee.Image('UMD/GLAD/PRIMARY_HUMID_TROPICAL_FORESTS/v1/2001')\n",
    "\n",
    "# select tree cover loss band and unmask with value 0 to represent no loss\n",
    "tclYear = forestChange.select(['lossyear']).unmask(0)\n",
    "\n",
    "# define mask of primary forest areas\n",
    "primaryForestValid = primaryForest.eq(1);\n",
    "# define mask for where Hansen data is valid\n",
    "forestChangeValid = forestChange.select('datamask').eq(1)\n",
    "# define mask of tree cover loss > 30%\n",
    "forestCoverValid = forestChange.select('treecover2000').gte(30)\n",
    "\n",
    "# create final mask where (primaryForestValid = 1) AND (forestChangeValid = 1) AND (forestCoverValid = 1)\n",
    "treeCoverLossValid = primaryForestValid.And(forestCoverValid).And(forestChangeValid)\n",
    "\n",
    "# update tree cover loss year to mask invalid areas\n",
    "tclYearMasked = tclYear.updateMask(treeCoverLossValid)\n",
    "# now tclYearMasked = {0 when tree cover loss did not occur, 1-19 for year when tree cover loss occured} only in valid areas\n",
    "\n",
    "\n",
    "\n",
    "center = [-1.776125,23.710125]\n",
    "zoom = 4\n",
    "tclPalette = ['#0051ff',\n",
    "            '#f7f4f9','#f7f4f9',\n",
    "            '#e7e1ef','#e7e1ef',\n",
    "            '#d4b9da','#d4b9da',\n",
    "            '#c994c7','#c994c7',\n",
    "            '#df65b0','#df65b0',\n",
    "            '#e7298a','#e7298a',\n",
    "            '#ce1256','#ce1256',\n",
    "            '#980043','#980043',\n",
    "            '#67001f','#67001f']\n",
    "tclViz = {'min': 0, 'max': 19, 'palette': tclPalette}\n",
    "Map1 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map1.addLayer(forestChangeValid.updateMask(forestChangeValid),\n",
    "              {'min': 0, 'max': 1, 'palette': ['#f9261b','#5a1bf9']},name='Hansen Valid')\n",
    "Map1.addLayer(forestCoverValid.updateMask(forestCoverValid),\n",
    "              {'min': 0, 'max': 1, 'palette': ['#f7f91b','#28ce4c']},name='Tree Cover > 30')\n",
    "Map1.addLayer(primaryForestValid.updateMask(primaryForestValid),\n",
    "              {'min': 0, 'max': 1, 'palette': ['#f9801b','#9d1bf9']},name='Primary Forest')\n",
    "Map1.addLayer(forestChange.select(['lossyear']),tclViz,name='Tree Cover Loss UnMasked')\n",
    "Map1.addLayer(tclYearMasked,tclViz,name='Tree Cover Loss Masked')\n",
    "Map1.addLayer(targetLoss,tclViz,name='Python Result')\n",
    "display(Map1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lossyear']\n"
     ]
    }
   ],
   "source": [
    "congoRate = eef.getRateOfClass(image=targetLoss,classValue=1,region=DRCBoundary,\n",
    "                         crs=crs,crsTransform=crsTransform)\n",
    "\n",
    "congoRateFeature = ee.FeatureCollection(ee.Feature(None,congoRate))\n",
    "\n",
    "\n",
    "export_rate_task = ee.batch.Export.table.toDrive(\n",
    "    collection=congoRateFeature, \n",
    "    folder='CongoDeforestation',\n",
    "    description = 'CongoRate',\n",
    "    fileNamePrefix = 'CongoRate',\n",
    "    fileFormat='CSV')\n",
    "export_rate_task.start()\n",
    "print(targetLoss.bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polling for task (id: WCTAKYPQQRURVLXSP7DPQK4W).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ls/m3fnvjqj1qs_hsp9k3s_q1h00000gp/T/ipykernel_5881/3073932276.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mexport_results_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Polling for task (id: {}).'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_results_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done with export.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Sample point locations that we will split into training, validation, and test sets\n",
    "numPoints = 30000\n",
    "point_locations = eef.getStratifiedSampleBandPoints(targetLoss, region=DRCBoundary, \n",
    "                                                       numPoints=numPoints, bandName='lossyear',seed=seed,\n",
    "                                                       geometries=True, projection=forestChange.projection())\n",
    "\n",
    "training_locations, validation_locations, test_locations = eef.splitTrainValidationTestStratified(featureCollection=point_locations, \n",
    "                                                                                                  stratifyColumn='lossyear', \n",
    "                                                                                                  trainSplit=0.6, \n",
    "                                                                                                  testSplit=0.5,\n",
    "                                                                                                 seed=seed)\n",
    "\n",
    "\n",
    "#Export these locations to an Earth Engine asset\n",
    "location_description = '{}_locations'\n",
    "location_assetID = 'users/writscdrivers/CongoDeforestation/TrainingPoints/{}_locations'\n",
    "\n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "    collection=training_locations, \n",
    "    description = location_description.format('training'), \n",
    "    assetId = location_assetID.format('training'))\n",
    "export_results_task.start()\n",
    "    \n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "    collection=validation_locations, \n",
    "    description = location_description.format('validation'), \n",
    "    assetId = location_assetID.format('validation'))\n",
    "export_results_task.start()\n",
    "\n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "    collection=test_locations, \n",
    "    description = location_description.format('test'), \n",
    "    assetId = location_assetID.format('test'))\n",
    "export_results_task.start()\n",
    " \n",
    "#Wait for last export to finish\n",
    "while export_results_task.active():\n",
    "    print('Polling for task (id: {}).'.format(export_results_task.id))\n",
    "    time.sleep(30)\n",
    "print('Done with export.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Image', 'bands': [{'id': 'lossyear', 'data_type': {'type': 'PixelType', 'precision': 'int', 'min': 0, 'max': 1}, 'crs': 'EPSG:4326', 'crs_transform': [0.00025, 0, -180, 0, -0.00025, 80]}]}\n",
      "['treecover2000']\n",
      "Polling for task (id: LWHOY6THSIHYBJPASXFICWP2).\n",
      "Polling for task (id: LWHOY6THSIHYBJPASXFICWP2).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ls/m3fnvjqj1qs_hsp9k3s_q1h00000gp/T/ipykernel_5881/3936627826.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mexport_results_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Polling for task (id: {}).'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_results_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done with export.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "elevation = ee.Image(\"CGIAR/SRTM90_V4\")\n",
    "huntingAreas = ee.FeatureCollection('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal/HuntingAreas')\n",
    "protectedAreas = ee.FeatureCollection('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal/ProtectedAreas')\n",
    "roads = ee.FeatureCollection('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal/Roads')\n",
    "ruralComplex = ee.Image('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal/RuralComplex')\n",
    "mining = ee.FeatureCollection('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal/ActiveMining')\n",
    "logging = ee.FeatureCollection('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal/ArtisinalLogging')\n",
    "concessions = ee.FeatureCollection('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal/Concessions')\n",
    "conflictLocations = ee.FeatureCollection('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal/ConflictLocations')\n",
    "localities = ee.FeatureCollection('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal/Localities')\n",
    "navigableRivers = ee.FeatureCollection('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal/NavigableRivers')\n",
    "\n",
    "location_assetID = 'users/writscdrivers/CongoDeforestation/TrainingPoints/{}_locations'\n",
    "\n",
    "maxDistance = 50000\n",
    "\n",
    "localitiesImage = eef.rasterizeVector(featureCollection=localities)\n",
    "localitiesDistance = eef.distanceToImage(image=localitiesImage,maxDistance=maxDistance)\n",
    "localitiesDistance = localitiesDistance.divide(maxDistance)\n",
    "\n",
    "conflictsImage = eef.rasterizeVector(featureCollection=conflictLocations)\n",
    "conflictsDistance = eef.distanceToImage(image=conflictsImage,maxDistance=maxDistance)\n",
    "conflictsDistance = conflictsDistance.divide(maxDistance)\n",
    "\n",
    "navigableRiversImage = eef.rasterizeVector(featureCollection=navigableRivers)\n",
    "navigableRiversDistance = eef.distanceToImage(image=navigableRiversImage,maxDistance=maxDistance)\n",
    "navigableRiversDistance = navigableRiversDistance.divide(maxDistance)\n",
    "\n",
    "loggingImage = eef.rasterizeVector(featureCollection=logging)\n",
    "loggingDistance = eef.distanceToImage(image=loggingImage,maxDistance=maxDistance)\n",
    "loggingDistance = loggingDistance.divide(maxDistance)\n",
    "\n",
    "miningImage = eef.rasterizeVector(featureCollection=mining)\n",
    "\n",
    "concessionsImage = eef.rasterizeVector(featureCollection=concessions)\n",
    "\n",
    "huntingAreasImage = eef.rasterizeVector(featureCollection=huntingAreas)\n",
    "\n",
    "protectedAreasImage = eef.rasterizeVector(featureCollection=protectedAreas)\n",
    "\n",
    "roadsImage = eef.rasterizeVector(featureCollection=roads)\n",
    "roadsDistance = eef.distanceToImage(image=roadsImage,maxDistance=maxDistance)\n",
    "roadsDistance = roadsDistance.divide(maxDistance)\n",
    "\n",
    "ruralComplexDistance = eef.distanceToImage(image=ruralComplex,maxDistance=maxDistance)\n",
    "ruralComplexDistance = ruralComplexDistance.divide(maxDistance)\n",
    "\n",
    "earlyLoss = eef.getTargetLoss(startYear=2001,endYear=2010,treeCoverThreshold=0,maskToPrimaryForest=False)\n",
    "earlyLossDistance = eef.distanceToImage(image=earlyLoss,maxDistance=maxDistance)\n",
    "earlyLossDistance = earlyLossDistance.divide(maxDistance)\n",
    "\n",
    "slope = ee.Terrain.slope(elevation)\n",
    "\n",
    "predictors = ['earlyLossDistance','elevation','slope','huntingAreasImage',\n",
    "                  'protectedAreasImage','roadsDistance','ruralComplexDistance',\n",
    "                  'localitiesDistance','conflictsDistance','navigableRiversDistance',\n",
    "                  'loggingDistance', 'miningImage', 'concessionsImage']\n",
    "predictorVariableImage = ee.ImageCollection([earlyLossDistance,\n",
    "                                                  elevation,\n",
    "                                                  slope,\n",
    "                                                  huntingAreasImage,\n",
    "                                                  protectedAreasImage,\n",
    "                                                  roadsDistance,\n",
    "                                                  ruralComplexDistance,\n",
    "                                                  localitiesDistance,\n",
    "                                                  conflictsDistance,\n",
    "                                                  navigableRiversDistance,\n",
    "                                                  loggingDistance,\n",
    "                                                  miningImage,\n",
    "                                                  concessionsImage])\n",
    "predictorVariableImage = predictorVariableImage.toBands().rename(predictors)\n",
    "\n",
    "trainingLocations = ee.FeatureCollection(location_assetID.format('training'))\n",
    "validationLocations = ee.FeatureCollection(location_assetID.format('validation'))\n",
    "testLocations = ee.FeatureCollection(location_assetID.format('test'))\n",
    "\n",
    "\n",
    "trainingPoints = predictorVariableImage.sampleRegions(\n",
    "  collection=trainingLocations, \n",
    "  scale=forestChange.projection().nominalScale().getInfo(), \n",
    "  tileScale=6, \n",
    "  geometries=True)\n",
    "validationPoints = predictorVariableImage.sampleRegions(\n",
    "  collection=validationLocations, \n",
    "  scale=forestChange.projection().nominalScale().getInfo(), \n",
    "  tileScale=6, \n",
    "  geometries=True)\n",
    "testPoints = predictorVariableImage.sampleRegions(\n",
    "  collection=testLocations, \n",
    "  scale=forestChange.projection().nominalScale().getInfo(), \n",
    "  tileScale=6, \n",
    "  geometries=True)\n",
    "\n",
    "export_results_task = ee.batch.Export.table.toDrive(\n",
    "    collection=trainingPoints, \n",
    "    folder='CongoDeforestation',\n",
    "    description = 'trainingPoints',\n",
    "    fileNamePrefix = 'trainingPoints',\n",
    "    fileFormat='CSV')\n",
    "export_results_task.start()\n",
    "\n",
    "export_results_task = ee.batch.Export.table.toDrive(\n",
    "    collection=validationPoints, \n",
    "    folder='CongoDeforestation',\n",
    "    description = 'validationPoints',\n",
    "    fileNamePrefix = 'validationPoints',\n",
    "    fileFormat='CSV')\n",
    "export_results_task.start()\n",
    "\n",
    "export_results_task = ee.batch.Export.table.toDrive(\n",
    "    collection=testPoints, \n",
    "    folder='CongoDeforestation',\n",
    "    description = 'testPoints',\n",
    "    fileNamePrefix = 'testPoints',\n",
    "    fileFormat='CSV')\n",
    "export_results_task.start()\n",
    "\n",
    "\n",
    "#Wait for last export to finish\n",
    "while export_results_task.active():\n",
    "    print('Polling for task (id: {}).'.format(export_results_task.id))\n",
    "    time.sleep(30)\n",
    "print('Done with export.')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Initialize earth engine\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception as e:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define seed number to reproduce results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seed = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Hansen forest change data and the DRC country boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Hansen forest change data\n",
    "forestChange = ee.Image(\"UMD/hansen/global_forest_change_2019_v1_7\")\n",
    "\n",
    "# save projection information\n",
    "projection_ee = forestChange.projection()\n",
    "projection = projection_ee.getInfo()\n",
    "crs = projection.get('crs')\n",
    "crsTransform = projection.get('transform')\n",
    "scale = projection_ee.nominalScale().getInfo()\n",
    "print('CRS and Transform: ',crs, crsTransform)\n",
    "\n",
    "# Load DRC feature\n",
    "countryBoundaries = ee.FeatureCollection('projects/resource-watch-gee/gadm36_0')\n",
    "DRCBoundary = countryBoundaries.filterMetadata('GID_0','equals','COD').first().geometry()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask tree cover loss to areas with greater than 30% tree cover in 2000 and in primary forest areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load primary forest data\n",
    "primaryForest = ee.Image('UMD/GLAD/PRIMARY_HUMID_TROPICAL_FORESTS/v1/2001')\n",
    "\n",
    "# select tree cover loss band and unmask with value 0 to represent no loss\n",
    "tclYear = forestChange.select(['lossyear']).unmask(0)\n",
    "\n",
    "# define mask of primary forest areas\n",
    "primaryForestValid = primaryForest.eq(1);\n",
    "# define mask for where Hansen data is valid\n",
    "forestChangeValid = forestChange.select('datamask').eq(1)\n",
    "# define mask of tree cover loss > 30%\n",
    "forestCoverValid = forestChange.select('treecover2000').gte(30)\n",
    "\n",
    "# create final mask where (primaryForestValid = 1) AND (forestChangeValid = 1) AND (forestCoverValid = 1)\n",
    "treeCoverLossValid = primaryForestValid.bitwiseAnd(forestCoverValid).bitwiseAnd(forestChangeValid)\n",
    "\n",
    "# update tree cover loss year to mask invalid areas\n",
    "tclYearMasked = tclYear.updateMask(treeCoverLossValid)\n",
    "# now tclYearMasked = {0 when tree cover loss did not occur, 1-19 for year when tree cover loss occured} only in valid areas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot masked tree cover loss to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = [-1.776125,23.710125]\n",
    "zoom = 4\n",
    "tclPalette = ['#0051ff',\n",
    "            '#f7f4f9','#f7f4f9',\n",
    "            '#e7e1ef','#e7e1ef',\n",
    "            '#d4b9da','#d4b9da',\n",
    "            '#c994c7','#c994c7',\n",
    "            '#df65b0','#df65b0',\n",
    "            '#e7298a','#e7298a',\n",
    "            '#ce1256','#ce1256',\n",
    "            '#980043','#980043',\n",
    "            '#67001f','#67001f']\n",
    "tclViz = {'min': 0, 'max': 19, 'palette': tclPalette}\n",
    "Map1 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map1.addLayer(forestChangeValid.updateMask(forestChangeValid),\n",
    "              {'min': 0, 'max': 1, 'palette': ['#f9261b','#5a1bf9']},name='Hansen Valid')\n",
    "Map1.addLayer(forestCoverValid.updateMask(forestCoverValid),\n",
    "              {'min': 0, 'max': 1, 'palette': ['#f7f91b','#28ce4c']},name='Tree Cover > 30')\n",
    "Map1.addLayer(primaryForestValid.updateMask(primaryForestValid),\n",
    "              {'min': 0, 'max': 1, 'palette': ['#f9801b','#9d1bf9']},name='Primary Forest')\n",
    "Map1.addLayer(forestChange.select(['lossyear']),tclViz,name='Tree Cover Loss UnMasked')\n",
    "Map1.addLayer(tclYearMasked,tclViz,name='Tree Cover Loss Masked')\n",
    "\n",
    "display(Map1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create three binary layers:\n",
    "1. Tree cover loss that occurred from 2001 through 2011, this is referred to as early loss, which we'll use to calculate the distance to previous loss\n",
    "2. Tree cover loss that occurred from 2018 through 2019, this is loss we wil use in evaluating our model\n",
    "3. Tree cover loss that occurred from 2012 through 2017, this is our reference loss which we will be modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mask when tree cover occurred from 2001 through 2011\n",
    "tclEarlyLoss = tclYearMasked.expression(\n",
    "    '(year>0 && year<12)', {\n",
    "      'year': tclYearMasked.select('lossyear')\n",
    "})\n",
    "# Define mask when tree cover occurred after 2017\n",
    "tclLaterLoss = tclYearMasked.expression(\n",
    "    '(year>17)', {\n",
    "      'year': tclYearMasked.select('lossyear')\n",
    "})\n",
    "\n",
    "# Define binary variable for tree cover loss that occurred from 2012 through 2017,\n",
    "# loss that occurred after 2017 is marked as 0\n",
    "tclReferenceLoss = tclYearMasked.expression(\n",
    "    '(year>11 && year<18) ? 1 : 0', {\n",
    "      'year': tclYearMasked.select('lossyear')\n",
    "})\n",
    "\n",
    "# Define mask for tree cover loss that occurred from 2012 through 2017\n",
    "referenceTreeCoverLossValid = treeCoverLossValid.bitwiseAnd(tclEarlyLoss.eq(0))\n",
    "\n",
    "# Mask tree cover loss years to get binary 0 for no tree cover loss and 1 for \n",
    "tclReferenceLoss = tclReferenceLoss.updateMask(referenceTreeCoverLossValid).gt(0)\n",
    "tclReferenceLoss = tclReferenceLoss.rename('loss')\n",
    "\n",
    "# Map layers to double check!\n",
    "Map2 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map2.addLayer(tclYearMasked,tclViz,name='Tree Cover Loss Masked')\n",
    "Map2.addLayer(tclEarlyLoss,{'min': 0, 'max': 1, 'palette': ['#f9261b','#5a1bf9']},name='Early Loss')\n",
    "Map2.addLayer(tclLaterLoss,{'min': 0, 'max': 1, 'palette': ['#f9261b','#5a1bf9']},name='Later Loss')\n",
    "Map2.addLayer(tclReferenceLoss,{'min': 0, 'max': 1, 'palette': ['#f9261b','#5a1bf9']},name='Historical Loss')\n",
    "display(Map2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather training, testing, and validation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll remove training points that are within a certain distance to the validation and test points\n",
    "distanceToFilter = 10000\n",
    "numPoints = 6000\n",
    "\n",
    "#Sample point locations that we will split into training, validation, and test sets\n",
    "point_locations = npv.getStratifiedSampleBandPoints(tclReferenceLoss, region=DRCBoundary, \n",
    "                                                       numPoints=numPoints, bandName='loss',seed=num_seed,\n",
    "                                                       geometries=True, projection=projection_ee)\n",
    "\n",
    "#Assign random value between 0 and 1 to split into training, validation, and test sets\n",
    "point_locations = point_locations.randomColumn(columnName='TrainingSplit', seed=num_seed)\n",
    "#First split training set from the rest, taking 70% of the points for training\n",
    "#Roughly 70% training, 30% for validation + testing\n",
    "training_split = 0.7\n",
    "training_locations = point_locations.filter(ee.Filter.lt('TrainingSplit', training_split))\n",
    "validation_and_test = point_locations.filter(ee.Filter.gte('TrainingSplit', training_split))\n",
    "\n",
    "#Define distance filter to remove training points within a certain distance of test points and validation points\n",
    "distFilter = ee.Filter.withinDistance(distance=distanceToFilter, leftField='.geo', rightField= '.geo', maxError= 1)\n",
    "join = ee.Join.inverted()\n",
    "training_locations = join.apply(training_locations, validation_and_test, distFilter);\n",
    "\n",
    "#Assign another random value between 0 and 1 to validation_and_test to split to validation and test sets\n",
    "validation_and_test = validation_and_test.randomColumn(columnName='ValidationSplit', seed=num_seed)\n",
    "#Of the 30% saved for validation + testing, half goes to validation and half goes to test\n",
    "#Meaning original sample will be 70% training, 15% validation, 15% testing\n",
    "validation_split = 0.5 \n",
    "validation_locations = validation_and_test.filter(ee.Filter.lt('ValidationSplit', validation_split))\n",
    "test_locations = validation_and_test.filter(ee.Filter.gte('ValidationSplit', validation_split))\n",
    "\n",
    "#Apply distance filter to remove validation points within a certain distance of test points\n",
    "validation_locations = join.apply(validation_locations, test_locations, distFilter);\n",
    "\n",
    "#Export these locations to an Earth Engine asset\n",
    "location_description = '{}_locations'\n",
    "location_assetID = 'users/listerkristineanne/CongoDeforestation/TrainingPoints_{}_locations'\n",
    "#location_assetID = 'users/listerkristineanne/CongoDeforestation/TrainingPoints/TrainingPoints_{}_locations'\n",
    "\n",
    "# export_results_task = ee.batch.Export.table.toAsset(\n",
    "#     collection=training_locations, \n",
    "#     description = location_description.format('training'), \n",
    "#     assetId = location_assetID.format('training'))\n",
    "# export_results_task.start()\n",
    "    \n",
    "# export_results_task = ee.batch.Export.table.toAsset(\n",
    "#     collection=validation_locations, \n",
    "#     description = location_description.format('validation'), \n",
    "#     assetId = location_assetID.format('validation'))\n",
    "# export_results_task.start()\n",
    "\n",
    "# export_results_task = ee.batch.Export.table.toAsset(\n",
    "#     collection=test_locations, \n",
    "#     description = location_description.format('test'), \n",
    "#     assetId = location_assetID.format('test'))\n",
    "# export_results_task.start()\n",
    " \n",
    "# #Wait for last export to finish\n",
    "# while export_results_task.active():\n",
    "#     print('Polling for task (id: {}).'.format(export_results_task.id))\n",
    "#     time.sleep(30)\n",
    "# print('Done with export.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load elevation\n",
    "elevation = ee.Image(\"CGIAR/SRTM90_V4\")\n",
    "# Calculate slope\n",
    "slope = ee.Terrain.slope(elevation)\n",
    "\n",
    "# Load roads data\n",
    "roads = ee.FeatureCollection('users/listerkristineanne/CongoDeforestation/PredictorVariables/HistoricalFinal/RDC_voirie')\n",
    "\n",
    "# Create cost map for measuring distance, for now we'll list crossing any pixel as the same weight\n",
    "# Generate a constant image with value 1, clip it to the bounds of the DRC\n",
    "cost = ee.Image.constant(1).clip(DRCBoundary.bounds())\n",
    "maxDistance=50000\n",
    "\n",
    "# Calculate distance to loss from 2001 to 2011\n",
    "# First unmask earlyLoss to get target pixels\n",
    "earlyLossUnmasked = tclEarlyLoss.unmask(0)\n",
    "distanceToEarlyLoss = cost.cumulativeCost(source= earlyLossUnmasked, maxDistance=maxDistance).rename('earlyLossDistance')\n",
    "distanceToEarlyLoss = distanceToEarlyLoss.unmask(maxDistance)\n",
    "\n",
    "# Burn roads feature collection to image\n",
    "roads = roads.map(lambda x: x.set({'constant':1}))\n",
    "roadsImage = roads.reduceToImage(['constant'], ee.Reducer.first())\n",
    "# Convert image to binary 0:1 if there is a road\n",
    "roadsImage = roadsImage.unmask(0)\n",
    "# Calculate distance to roads\n",
    "distanceToRoads = cost.cumulativeCost(source= roadsImage, maxDistance=maxDistance).rename('roadsDistance')\n",
    "distanceToRoads = distanceToRoads.unmask(maxDistance)\n",
    "\n",
    "# Load rural complex\n",
    "ruralComplex = ee.Image('users/listerkristineanne/CongoDeforestation/PredictorVariables/HistoricalFinal/DRC_2010_Rural_Complex')\n",
    "# Calculate distance to rural complex\n",
    "distanceToRuralComplex = cost.cumulativeCost(source= ruralComplex, maxDistance=maxDistance).rename('ruralComplexDistance')\n",
    "distanceToRuralComplex = distanceToRuralComplex.unmask(maxDistance)\n",
    "\n",
    "# Load protected areas\n",
    "protectedAreas = ee.FeatureCollection('users/listerkristineanne/CongoDeforestation/PredictorVariables/HistoricalFinal/RDC_aire_protegee')\n",
    "# Filter hunting areas to another feature collection\n",
    "huntingAreas = protectedAreas.filterMetadata('REGLEMENT','equals','Domaine de chasse')\n",
    "# Filter protected areas to remove hunting areas\n",
    "protectedAreasFiltered = protectedAreas.filterMetadata('REGLEMENT','not_equals','Domaine de chasse')\n",
    "\n",
    "# Burn protected areas feature collection to image\n",
    "protectedAreasFiltered = protectedAreasFiltered.map(lambda x: x.set({'constant':1}))\n",
    "protectedAreasImage = protectedAreasFiltered.reduceToImage(['constant'], ee.Reducer.first())\n",
    "# Convert image to binary 0:1 if there is a road\n",
    "protectedAreasImage = protectedAreasImage.unmask(0)\n",
    "# Calculate distance to roads\n",
    "distanceToProtectedAreas = cost.cumulativeCost(source= protectedAreasImage, maxDistance=maxDistance).rename('protectedAreasDistance')\n",
    "distanceToProtectedAreas = distanceToProtectedAreas.unmask(maxDistance)\n",
    "\n",
    "\n",
    "# Burn hunting areas feature collection to image\n",
    "huntingAreas = huntingAreas.map(lambda x: x.set({'constant':1}))\n",
    "huntingAreasImage = huntingAreas.reduceToImage(['constant'], ee.Reducer.first())\n",
    "# Convert image to binary 0:1 if there is a road\n",
    "huntingAreasImage = huntingAreasImage.unmask(0)\n",
    "# Calculate distance to roads\n",
    "distanceToHuntingAreas = cost.cumulativeCost(source= huntingAreasImage, maxDistance=maxDistance).rename('huntingAreasDistance')\n",
    "distanceToHuntingAreas = distanceToHuntingAreas.unmask(maxDistance)\n",
    "\n",
    "# Define list of predictor variable images and convert to a single image\n",
    "predictor_variable_list = [distanceToEarlyLoss,distanceToRuralComplex,distanceToProtectedAreas,\n",
    "                           distanceToHuntingAreas,\n",
    "                           distanceToRoads,elevation,slope]\n",
    "predictor_variable_image = ee.ImageCollection(predictor_variable_list).toBands()\n",
    "\n",
    "# Rename bands\n",
    "predictor_variable_names = ['earlyLossDistance','ruralComplexDistance','roadsDistance',\n",
    "                            'protectedAreasDistance','huntingAreasDistance','elevation','slope']\n",
    "predictor_variable_image = predictor_variable_image.rename(predictor_variable_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old distance\n",
    "# roads = ee.FeatureCollection('users/listerkristineanne/CongoDeforestation/PredictorVariables/HistoricalFinal/RDC_voirie')\n",
    "\n",
    "# # Define kernel to calculate distance, we'll only calculate distance for features withint 500km\n",
    "# kernel = ee.Kernel.euclidean(500000,\"meters\")\n",
    "\n",
    "# # Calculate distance to loss from 2001 to 2011\n",
    "# distanceToEarlyLoss = tclEarlyLoss.unmask(0).distance(kernel,True).clip(DRCBoundary).rename('earlyLossDistance')\n",
    "\n",
    "# # Burn roads feature collection to image\n",
    "# roadsImage = roads.reduceToImage(['long_m'], ee.Reducer.first())\n",
    "# # Convert image to binary 0:1 if there is a road\n",
    "# roadsImage = roadsImage.gt(0)\n",
    "# # Calculate distance to roads\n",
    "# distanceToRoads = roadsImage.distance(kernel,True).clip(DRCBoundary).rename('roadsDistance')\n",
    "\n",
    "# # Define list of predictor variable images and convert to a single image\n",
    "# predictor_variable_list = [distanceToEarlyLoss,\n",
    "#                            distanceToRoads]\n",
    "# predictor_variable_image = ee.ImageCollection(predictor_variable_list).toBands()\n",
    "\n",
    "# # Rename bands t\n",
    "# predictor_variable_names = ['earlyLossDistance','roadsDistance']\n",
    "# predictor_variable_image = predictor_variable_image.rename(predictor_variable_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample predictor variables at training, validation, and test points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define properties to export to an Earth Engine asset\n",
    "points_description = '{}_points'\n",
    "points_assetID = 'users/listerkristineanne/CongoDeforestation/TrainingPoints_{}_points'\n",
    "\n",
    "# Load the point locations assets\n",
    "training_locations_asset = ee.FeatureCollection(location_assetID.format('training'))\n",
    "validation_locations_asset = ee.FeatureCollection(location_assetID.format('validation'))\n",
    "test_locations_asset = ee.FeatureCollection(location_assetID.format('test'))\n",
    "\n",
    "# Sample predictor variable at location\n",
    "training_points = predictor_variable_image.sampleRegions(training_locations_asset, \n",
    "                                                         projection=projection_ee, geometries=True,tileScale=16)\n",
    "validation_points = predictor_variable_image.sampleRegions(validation_locations_asset, \n",
    "                                                           projection=projection_ee, geometries=True,tileScale=16)\n",
    "test_points = predictor_variable_image.sampleRegions(test_locations_asset, \n",
    "                                                     projection=projection_ee, geometries=True,tileScale=16)\n",
    "\n",
    "# # # Export results\n",
    "# export_results_task = ee.batch.Export.table.toAsset(\n",
    "#     collection=training_points, \n",
    "#     description = points_description.format('training'), \n",
    "#     assetId = points_assetID.format('training'))\n",
    "# export_results_task.start()\n",
    "\n",
    "# export_results_task = ee.batch.Export.table.toAsset(\n",
    "#     collection=validation_points, \n",
    "#     description = points_description.format('validation'), \n",
    "#     assetId = points_assetID.format('validation'))\n",
    "# export_results_task.start()\n",
    "\n",
    "# export_results_task = ee.batch.Export.table.toAsset(\n",
    "#     collection=test_points, \n",
    "#     description = points_description.format('test'), \n",
    "#     assetId = points_assetID.format('test'))\n",
    "# export_results_task.start()\n",
    "\n",
    "# #Wait for last export to finish\n",
    "# while export_results_task.active():\n",
    "#     print('Polling for task (id: {}).'.format(export_results_task.id))\n",
    "#     time.sleep(30)\n",
    "# print('Done with export.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model parameters to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define dictionaries of parameters and models to test\n",
    "#You can find the inputs for the parameters under the ee.Classifiers section of GEE\n",
    "\n",
    "rf_parameters = {'seed':[num_seed], \n",
    "          'numberOfTrees': [50,100], \n",
    "          'variablesPerSplit': [1,2,None], \n",
    "          'minLeafPopulation': [4,10,50], \n",
    "          'bagFraction': [None,0.5,.3], \n",
    "          'maxNodes': [None, 20, 50]\n",
    "         }\n",
    "#buildGridSearchList converts the parameter dictionary into a list of classifiers that can be used in cross-validation\n",
    "rf_classifier_list = gclass.buildGridSearchList(rf_parameters,'smileRandomForest')\n",
    "\n",
    "svm_parameters = {'decisionProcedure':[None]}\n",
    "svm_classifier_list = gclass.buildGridSearchList(svm_parameters,'libsvm')\n",
    "\n",
    "#maxent_parameters = {'minIterations':[10,100],'maxIterations':[50,200]}\n",
    "#maxent_classifier_list = gclass.buildGridSearchList(maxent_parameters,'gmoMaxEnt')\n",
    "\n",
    "classifier_list = rf_classifier_list+svm_classifier_list#+maxent_classifier_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cross-validation to test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Name y_column\n",
    "y_column = 'loss'\n",
    "\n",
    "#Define assetId and description format to export to GEE\n",
    "cv_results_assetId = 'users/listerkristineanne/CongoDeforestation/TrainingPoints/CV_Results_20210305'\n",
    "cv_results_description = 'cv_export_20210305'\n",
    "\n",
    "#Load training points\n",
    "training_points = ee.FeatureCollection(points_assetID.format('training'))\n",
    "\n",
    "#Perform cross validation, returns a feature collection\n",
    "cv_results = gclass.kFoldCrossValidation(inputtedFeatureCollection = training_points, \n",
    "                                     propertyToPredictAsString = y_column, \n",
    "                                     predictors = predictor_variable_names, \n",
    "                                     listOfClassifiers = classifier_list,\n",
    "                                     k=3,seed=num_seed)\n",
    "#Export results to GEE\n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "        collection=cv_results, \n",
    "        description = cv_results_description, \n",
    "        assetId = cv_results_assetId)\n",
    "export_results_task.start()\n",
    "\n",
    "#Wait for export to finish\n",
    "while export_results_task.active():\n",
    "    print('Polling for task (id: {}).'.format(export_results_task.id))\n",
    "    time.sleep(30)\n",
    "print('Done with export.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate accuracy of models on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty dataframe to save results of cross validation\n",
    "accuracy_and_keys = pd.DataFrame()\n",
    "\n",
    "#Load cross-validation results\n",
    "results = ee.FeatureCollection(cv_results_assetId)\n",
    "#Get the best result by the cross validation score\n",
    "best_result = results.sort('Validation Score', False).first()\n",
    "\n",
    "#Load params as a dictionary\n",
    "params = best_result.get('Params').getInfo()\n",
    "params = ast.literal_eval(params)\n",
    "\n",
    "#Get the calssifier name\n",
    "classifierName = best_result.get('Classifier Type').getInfo()\n",
    "\n",
    "#Load classifier with best params\n",
    "best_model = gclass.defineClassifier(params,classifierName)\n",
    "\n",
    "#Load training and validation points\n",
    "training_points = ee.FeatureCollection(points_assetID.format('training'))\n",
    "validation_points = ee.FeatureCollection(points_assetID.format('validation'))\n",
    "\n",
    "#Train a classifier with the best params on the training data\n",
    "best_model = best_model.train(training_points, classProperty=y_column, \n",
    "                              inputProperties=predictor_variable_names, subsamplingSeed=num_seed)\n",
    "\n",
    "#Predict over validation data\n",
    "validation_points_predicted = validation_points.classify(best_model)\n",
    "\n",
    "#Get confusion matrix and accuracy score\n",
    "confusion_matrix = validation_points_predicted.errorMatrix(y_column, 'classification');\n",
    "accuracy = confusion_matrix.accuracy().getInfo()\n",
    "\n",
    "\n",
    "#Get confusion matrix and accuracy score\n",
    "confusion_matrix = validation_points_predicted.errorMatrix(y_column, 'classification');\n",
    "accuracy = confusion_matrix.accuracy().getInfo()\n",
    "print('--------------------------------------------------------------')\n",
    "print('Final Model Validation Set Confusion Matrix')\n",
    "print(gclass.pretty_print_confusion_matrix_binary(confusion_matrix.getInfo()))\n",
    "print('Final Model Validation Set Accuracy',accuracy)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# #Predict over test data\n",
    "# test_points = ee.FeatureCollection(points_assetID.format('test'))\n",
    "\n",
    "# test_points_predicted = test_points.classify(best_model)\n",
    "\n",
    "# #Get confusion matrix and accuracy score\n",
    "# confusion_matrix = test_points_predicted.errorMatrix(y_column, 'classification');\n",
    "# accuracy = confusion_matrix.accuracy().getInfo()\n",
    "# print('--------------------------------------------------------------')\n",
    "# print('Final Model Test Set Confusion Matrix')\n",
    "# print(gclass.pretty_print_confusion_matrix_binary(confusion_matrix.getInfo()))\n",
    "# print('Final Model Test Set Accuracy',accuracy)\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model.schema().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use best model and predictor variable image to predict loss\n",
    "predicted_loss = predictor_variable_image.updateMask(referenceTreeCoverLossValid).classify(best_model)\n",
    "\n",
    "# Map layers to double check!\n",
    "Map3 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "#Map3.addLayer(tclYearMasked,tclViz,name='Tree Cover Loss Masked')\n",
    "Map3.addLayer(tclReferenceLoss,{'min': 0, 'max': 1, 'palette': ['#98BC47','#DA6E9A']},name='Historical Loss')\n",
    "Map3.addLayer(predicted_loss,{'min': 0, 'max': 1, 'palette': ['#98BC47','#DA6E9A']},name='Predicted Loss')\n",
    "#Map3.addLayer(tclReferenceLoss.eq(predicted_loss),{'min': 0, 'max': 1, 'palette': ['#98BC47','#DA6E9A']},name='Accurate Prediction')\n",
    "\n",
    "display(Map3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket = 'wri-congo-deforestation'\n",
    "# fileFormat = 'CSV'\n",
    "\n",
    "# #Export.table.toCloudStorage(collection, description, bucket, fileNamePrefix, fileFormat, selectors)\n",
    "\n",
    "# # # Export results\n",
    "# export_results_task = ee.batch.Export.table.toCloudStorage(\n",
    "#     collection=training_points, \n",
    "#     description = points_description.format('training'), \n",
    "#     bucket = bucket,\n",
    "#     fileFormat = fileFormat,\n",
    "#     fileNamePrefix = points_description.format('training'))\n",
    "# export_results_task.start()\n",
    "\n",
    "# export_results_task = ee.batch.Export.table.toCloudStorage(\n",
    "#     collection=training_points, \n",
    "#     description = points_description.format('validation'), \n",
    "#     bucket = bucket,\n",
    "#     fileFormat = fileFormat,\n",
    "#     fileNamePrefix = points_description.format('validation'))\n",
    "# export_results_task.start()\n",
    "\n",
    "# export_results_task = ee.batch.Export.table.toCloudStorage(\n",
    "#     collection=training_points, \n",
    "#     description = points_description.format('test'), \n",
    "#     bucket = bucket,\n",
    "#     fileFormat = fileFormat,\n",
    "#     fileNamePrefix = points_description.format('test'))\n",
    "# export_results_task.start()\n",
    "\n",
    "# #Wait for last export to finish\n",
    "# while export_results_task.active():\n",
    "#     print('Polling for task (id: {}).'.format(export_results_task.id))\n",
    "#     time.sleep(30)\n",
    "# print('Done with export.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
